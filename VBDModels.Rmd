VBD Models notebook

```{r Libraries}
library(ggplot2)
library(brms)
library(dplyr)
library(tidyverse)
library(MASS)
library(boot) # For LOOCV
library(randomForest)
library(lme4)
library(epiCo)
library(incidence)
library(factoextra)
library(caret)
library(ciTools)


#install.packages("pak")
#pak::pak("epiverse-trace/sivirep")
```






```{r Random Forest}

df1 = read.csv('/Users/maxbeal/Desktop/PhD/Amazon/Data/LeadTimePreds/Predictors_6MLead_incidence')


df=df1[,c("year","month","variable","value","city","cases")]

df=na.omit(df)

pdf = pivot_wider(df, names_from = variable, values_from = value)
names(pdf) <- gsub("[ .+()]", "", names(pdf))

beta_estimates <- data.frame(City = character(),
                             Month = integer(),
                             Variable = character(),
                             Beta = numeric(),
                             stringsAsFactors = FALSE)

results <- data.frame(City = character(),
                             Month = integer(),
                             Predictions = numeric(),
                              Observations = numeric(),
                             stringsAsFactors = FALSE)
cv_results_list <- list()

pdf
cities <- c("cali", "cucuta", "leticia", "medellin")
months <- 1:12  # Assuming data includes all 12 months

for (cit in cities) {
  for (mon in months) {
    # Subset data for the current city and month
    city_month_data <- pdf %>%
      dplyr::filter(city == cit & month == mon)
    
    clean_data <- city_month_data[, colSums(is.na(city_month_data)) <= 2]

    #clean_data$cases = round(clean_data$cases)
    
    if (ncol(clean_data)<=3) {
      next()
    }
    
    clean_data=clean_data %>% dplyr::select(-year,-month,-city)
    correlations <- cor(clean_data[, -which(names(clean_data) == "cases")], clean_data$cases)

    # Set significance level
    alpha <- 0.05
    
    # Loop through each column
    for (col in names(correlations)) {
      # Test if correlation is significant
      p_value <- cor.test(clean_data[, col], clean_data$cases)$p.value
      
      # If p-value is greater than alpha, drop the predictor
      if (p_value > alpha) {
        clean_data <- select(clean_data, -col)
        cat("Column", col, "dropped due to non-significant correlation with 'cases'.\n")
      }
    }
    
    
    
    yhats=c()
    for (i in 1:nrow(clean_data)) {
        
      test = clean_data[i,]
      train=clean_data[-i,]
      
        # Fit negative binomial regression model in LOOCV
        model <- tryCatch({
            randomForest(cases ~ ., data = train)
              }, warning = function(w) {
                warning(paste("Warning:", w))
                NULL
              }, error = function(e) {
                warning(paste("Error:", e))
                NULL
              })
        
        #varImpPlot(model)
        
        if(!is.null(model)){
          yhats[i] = predict(model,newdata=test)
        }

    }
    
    

    #betas
    if(!is.null(model)){
      
      beta_df <- data.frame(City = cit,
                          Month = mon,
                          Variable = rownames(model$importance),
                          Beta = model$importance,
                          stringsAsFactors = FALSE)
      beta_estimates <- rbind(beta_estimates, beta_df)
      
      
      out=data_frame(City=cit,
                 Month=mon,
                 Predictions=yhats,
                 Observations=clean_data$cases,
                 stringsAsFactors=FALSE)
      
      results <- rbind(results,out)
      
    }
    
    # Print model summary
    cat("City:", cit, "Month:", mon, "\n")
    #print(summary(model))
    
    # You can also store the model if needed
    # models[[paste(city, month, sep="_")]] <- model
  }
}


r2 <- function(observations, predictions) {
  # Calculate mean of observations
  mean_obs <- mean(observations)
  # Calculate total sum of squares
  total_sum_squares <- sum((observations - mean_obs)^2)
  # Calculate residual sum of squares
  residual_sum_squares <- sum((observations - predictions)^2)
  # Calculate R-squared
  r_squared <- 1 - (residual_sum_squares / total_sum_squares)
  return(r_squared)
}


r_squared <- results %>%
  dplyr::group_by(City) %>%
  summarize(r_sq = r2(Observations, Predictions))

ggplot(results, aes(x = Observations, y = Predictions, color = City)) +
  geom_point(size=0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + # Add a 1:1 line for reference
  labs(x = "Observations", y = "Predictions", title = "Observations vs Predictions by Month") +
  theme_minimal()+
  geom_text(data = r_squared[1,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 130),check_overlap = T)+
  geom_text(data = r_squared[2,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 120),check_overlap = T)+
  geom_text(data = r_squared[3,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 110),check_overlap = T)+
  geom_text(data = r_squared[4,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 100),check_overlap = T)+
  facet_wrap(~ City, ncol = 2) + theme(aspect.ratio = 1) + ylim(0,50) + xlim(0,50)

r_squared <- results %>%
  dplyr::group_by(City,Month) %>%
  summarize(r_sq = r2(Observations,Predictions))

ggplot(results, aes(x = Observations, y = Predictions, color = Month)) +
  geom_point(size=0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + # Add a 1:1 line for reference
  labs(x = "Observations", y = "Predictions", title = "Observations vs Predictions by City") +
  theme_minimal() + facet_wrap(~Month,ncol=6) + theme(aspect.ratio = 1) + ylim(0,50) + xlim(0,50)


ggplot(r_squared,aes(y=r_sq,x=Month,group=City,color=City)) + geom_point() + geom_line() + ylim(-0.1,1)




```






```{r All in One Model, warning=FALSE}


library(caret)
df1 = read.csv('/Users/maxbeal/Desktop/PhD/Amazon/Data/LeadTimePreds/Predictors_3MLead_incidence')


df1 <- df1[complete.cases(df1), ]

df1$month = as.factor(df1$month)
df1$city = as.factor(df1$city)

df1=df1 %>% dplyr::select(-X)

df1[df1$cases<0,] = 0

pdf = pivot_wider(df1, names_from = variable, values_from = value)


remove_na_columns <- function(df, percent) {
  na_percent <- colMeans(is.na(df))
  columns_to_remove <- names(na_percent[na_percent > percent])
  df <- df[, !names(df) %in% columns_to_remove]
  return(df)
}

pdf = remove_na_columns(pdf,0.4)
pdf=na.omit(pdf)

year = pdf$year

pdf = pdf %>% dplyr::select(-year)

pdf$cases = log(pdf$cases + 0.01)

num_folds=5
folds <- createFolds(pdf$cases, k = num_folds, list = TRUE, returnTrain = FALSE)

results=list()
# Loop through each fold
for (i in 1:num_folds) {
  # Get the indices for train and test sets for the current fold
  train_indices <- unlist(folds[i])
  test_indices <- setdiff(1:nrow(pdf), train_indices)
  
  # Subset the dataframe into train and test sets
  train_set <- pdf[train_indices, ]
  test_set <- pdf[test_indices, ]
  
  
  #model <- glm.nb(cases ~ TMAX  + (1|city), data = train_set)
  #model <- glm(cases ~ ., data = train_set,family=poisson())
  
  #train_set
  model <- lmer(cases ~ . + (1|month) + (1|city), data = train_set)
  #glmmTMB(cases ~ TMAX + (1|month) + (1|city), data = train_set)
  
  Predictions = predict(model,newdata=test_set)
  
  Predictions[Predictions<0] = 0
  r=data.frame("City"=test_set$city,"Month"=test_set$month,"Year"=year[test_indices],"Observations"=test_set$cases,"Predictions"=Predictions)
  
  results[[i]] = r
  
}


print(colnames(pdf))

results <- do.call(rbind, results)

r2 <- function(observations, predictions) {
  # Calculate mean of observations
  mean_obs <- mean(observations)
  # Calculate total sum of squares
  total_sum_squares <- sum((observations - mean_obs)^2)
  # Calculate residual sum of squares
  residual_sum_squares <- sum((observations - predictions)^2)
  # Calculate R-squared
  r_squared <- 1 - (residual_sum_squares / total_sum_squares)
  return(r_squared)
}


r_squared <- results %>%
  dplyr::group_by(City) %>%
  summarize(r_sq = r2(Observations, Predictions))

ggplot(results, aes(x = exp(Observations), y = exp(Predictions), color = City)) +
  geom_point(size=0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + # Add a 1:1 line for reference
  labs(x = "Observations", y = "Predictions", title = "Observations vs Predictions by Month") +
  theme_minimal()+
  geom_text(data = r_squared[1,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 130),check_overlap = T)+
  geom_text(data = r_squared[2,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 120),check_overlap = T)+
  geom_text(data = r_squared[3,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 110),check_overlap = T)+
  geom_text(data = r_squared[4,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 100),check_overlap = T)+
  facet_wrap(~ City, ncol = 2) + theme(aspect.ratio = 1) + ylim(0,50) + xlim(0,50)

r_squared <- results %>%
  dplyr::group_by(City,Month) %>%
  summarize(r_sq = r2(Observations,Predictions))

ggplot(results, aes(x = exp(Observations), y = exp(Predictions), color = Month)) +
  geom_point(size=0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + # Add a 1:1 line for reference
  labs(x = "Observations", y = "Predictions", title = "Observations vs Predictions by City") +
  theme_minimal() + facet_wrap(~Month,ncol=6) + theme(aspect.ratio = 1) + ylim(0,50) + xlim(0,50)


ggplot(r_squared,aes(y=r_sq,x=Month,group=City,color=City)) + geom_point() + geom_line() + ylim(-0.1,1)



```
```{r}


```


```{r Negative Binomial, warning=FALSE}
df1 = read.csv('/Users/maxbeal/Desktop/PhD/Amazon/Data/LeadTimePreds/Predictors_6MLead_incidence')

ggplot(df1, aes(x = cases)) +
    geom_histogram(colour = "#8B5A00", fill = "#CD8500") +
    theme_bw() +
    ylab("Count\n") +
    xlab("DENV Incidence") +
    theme(axis.text = element_text(size = 12),
          axis.title = element_text(size = 14, face = "plain"))

log_transform=FALSE

df=df1[,c("year","month","variable","value","city")]

dfc=df1[,c("year","month","cases","city")]

#df=na.omit(df)

pdf = pivot_wider(df, names_from = variable, values_from = value)
names(pdf) <- gsub("[ .+()]", "", names(pdf))

df_cases = dfc %>% group_by(city,month,year) %>% summarise(cases=mean(cases,na.rm=T))

pdf=merge(pdf,df_cases)


beta_estimates <- data.frame(City = character(),
                             Month = integer(),
                             Variable = character(),
                             Beta = numeric(),
                             stringsAsFactors = FALSE)

results <- data.frame(City = character(),
                             Month = integer(),
                             Predictions = numeric(),
                              Observations = numeric(),
                             stringsAsFactors = FALSE)
cv_results_list <- list()

pdf
cities <- c("cali", "cucuta", "leticia", "medellin")
months <- 1:12  # Assuming data includes all 12 months
VarImpStore=list()
st=1
for (cit in cities) {
  for (mon in months) {
    # Subset data for the current city and month
    city_month_data <- pdf %>%
      dplyr::filter(city == cit & month == mon)
    
    city_month_data = city_month_data %>% group_by(year,month,city) %>% summarise_all(~ mean(.x, na.rm = TRUE))
    
    clean_data <- city_month_data[, colSums(is.na(city_month_data)) <= 7]
    
    #print(colnames(clean_data))
    print(nrow(clean_data))
    

    
    if (ncol(clean_data)<=3) {
      next()
    }
    
    clean_data=na.omit(clean_data)
    years = clean_data$year
    clean_data=clean_data %>% ungroup() %>% dplyr::select(-year,-month,-city)
    
    
    correlations <- cor(clean_data[, -which(names(clean_data) == "cases")], clean_data$cases)

    # Set significance level
    alpha <- 0.05
    
    # Loop through each column
    for (col in names(correlations)) {
      # Test if correlation is significant
      p_value <- cor.test(clean_data[, col], clean_data$cases)$p.value
      
      # If p-value is greater than alpha, drop the predictor
      if (p_value > alpha) {
        clean_data <- select(clean_data, -col)
        cat("Column", col, "dropped due to non-significant correlation with 'cases'.\n")
      }
    }
    

    df_pca = prcomp(clean_data %>% dplyr::select(-cases))
    
    
    pca_sum = summary(df_pca)
    #Proportion of variance
    PC_keep = names(pca_sum$importance[2,] > 0.1)[pca_sum$importance[2,] > 0.1]
    
    PCs = df_pca$x[,colnames(df_pca$x) %in% PC_keep]
    
    PCcon=get_pca_var(df_pca)
    
    PCs=data.frame(PCs)
    colnames(PCs)[1] = "PC1"

    VarImp=data.frame(PCcon$contrib[,1:ncol(PCs)])
    colnames(VarImp) = colnames(PCs)
    VarImpStore[[st]] = VarImp
    st = st+1
    
    

    if (log_transform==TRUE) {
      transformed_data = data.frame(cbind(log(clean_data$cases+0.01),PCs))
      colnames(transformed_data)[1] = "cases"
      
    }
    else{
    transformed_data = data.frame(cbind(clean_data$cases,PCs))
    colnames(transformed_data)[1] = "cases"
    }
    
    ggplot(transformed_data, aes(x = cases)) +
    geom_histogram(colour = "#8B5A00", fill = "#CD8500") +
    theme_bw() +
    ylab("Count\n") +
    xlab("DENV Incidence") +
    theme(axis.text = element_text(size = 12),
          axis.title = element_text(size = 14, face = "plain"))
  
    yhats=c()
    ys=c()
    ap05=c()
    ap95=c()
    for (i in 1:nrow(transformed_data)) {
        
      test = transformed_data[i,]
      train=transformed_data[-i,]
      
      # Fit negative binomial regression model in LOOCV
      model <- glm.nb(cases ~ ., data = train)
      
      
      #  #1. Linearity of predictors
      # # Plot the observed vs. fitted values
      # fitted <- fitted(model)
      # ggplot(train, aes(x = fitted, y = cases)) +
      #   geom_point() +
      #   geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
      #   labs(x = "Fitted Values", y = "Observed Values", title = "Observed vs Fitted Values") +
      #   theme_minimal()
      # 
      # # 2. Overdispersion
      # # Check for overdispersion by exam the relationship between the squared Pearson residuals and the fittedvalues
      # residuals <- residuals(model, type = "pearson")
      # fitted_values <- fitted(model)
      # ggplot(train, aes(x = fitted_values, y = residuals^2)) +
      #   geom_point() +
      #   geom_smooth(method = "loess", se = FALSE, color = "red") +
      #   labs(x = "Fitted Values", y = "Squared Pearson Residuals", title = "Overdispersion Check") +
      #   theme_minimal()
      # 

        
        if(!is.null(model)){
          yhats[i] = predict(model,newdata=test,type="response")
          PI = add_pi(test, model, names = c("ap05", "ap95"))
          ap05[i] = PI$ap05
          ap95[i] = PI$ap95
          ys[i] = test$cases
        }

    }
    
    

    #betas
    if(!is.null(model)){
      
      beta_df <- data.frame(City = cit,
                          Month = mon,
                          Variable = names(model$coefficients),
                          Beta = model$coefficients,
                          stringsAsFactors = FALSE)
      beta_estimates <- rbind(beta_estimates, beta_df)
      
      
      out=data_frame(City=cit,
                 Month=mon,
                 year =years,
                 Predictions=yhats,
                 ap05=ap05,
                 ap95=ap95,
                 Observations=ys)
      
      results <- rbind(results,out)
      
    }
    
    # Print model summary
    cat("City:", cit, "Month:", mon, "\n")
    print(nrow(out))
    #print(summary(model))
    
    # You can also store the model if needed
    # models[[paste(city, month, sep="_")]] <- model
  }
}


r2 <- function(observations, predictions) {
  # Calculate mean of observations
  mean_obs <- mean(observations)
  # Calculate total sum of squares
  total_sum_squares <- sum((observations - mean_obs)^2)
  # Calculate residual sum of squares
  residual_sum_squares <- sum((observations - predictions)^2)
  # Calculate R-squared
  r_squared <- 1 - (residual_sum_squares / total_sum_squares)
  return(r_squared)
}


if (log_transform==TRUE) {
  results$Observations = exp(results$Observations)
  results$Predictions = exp(results$Predictions)
}

r_squared <- results %>%
  dplyr::group_by(City,Month) %>%
  summarize(r_sq = r2(Observations, Predictions))

ggplot(results, aes(x = Observations, y = Predictions, color = City)) +
  geom_point(size=0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + # Add a 1:1 line for reference
  labs(x = "Observations", y = "Predictions", title = "Observations vs Predictions by Month") +
  theme_minimal()+
  geom_text(data = r_squared[1,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 130),check_overlap = T)+
  geom_text(data = r_squared[2,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 120),check_overlap = T)+
  geom_text(data = r_squared[3,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 110),check_overlap = T)+
  geom_text(data = r_squared[4,], aes(label = paste("R^2 =", round(r_sq, 2)), x = 100, y = 100),check_overlap = T)+
  facet_wrap(~ City, ncol = 2) + theme(aspect.ratio = 1) + ylim(0,50) + xlim(0,50)

r_squared <- results %>%
  dplyr::group_by(City,Month) %>%
  summarize(r_sq = r2(Observations,Predictions))

ggplot(results, aes(x = Observations, y = Predictions, color = Month)) +
  geom_point(size=0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + # Add a 1:1 line for reference
  labs(x = "Observations", y = "Predictions", title = "Observations vs Predictions by City") +
  theme_minimal() + facet_wrap(~Month,ncol=6) + theme(aspect.ratio = 1) + ylim(0,50) + xlim(0,50)


ggplot(results, aes(x = Observations, y = Predictions, color = Month)) +
  geom_point(size=0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + # Add a 1:1 line for reference
  labs(x = "Observations", y = "Predictions", title = "Observations vs Predictions by City") +
  theme_minimal() + facet_wrap(City~Month,ncol=12) + theme(aspect.ratio = 1) + ylim(0,50) + xlim(0,50)


ggplot(r_squared,aes(y=r_sq,x=Month,group=City,fill=City)) + geom_bar(stat = "Identity",position="dodge") + ylim(-1,1)



library(purrr)
library(dplyr)
vi<-VarImpStore %>%
    map_dfr(~as.data.frame(t(as.matrix(.)))) %>%
    rename_with(~sub("V", "column",.))


viMean=colMeans(vi,na.rm=T)
vi$PC = substr(rownames(vi),1,3)

library(reshape2)
vi = melt(vi, id = c("PC")) 

ggplot(vi,aes(x=variable, y= value)) + geom_boxplot() + facet_wrap(~PC)+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


#MAtch with python
colnames(results) = c("city","month","year","preds","ap25","ap75","cases")

hist(results$cases)

write.csv(results,'/Users/maxbeal/Desktop/PhD/Amazon/ModelResults/ModelResults_6mo_nb.csv') #CHANGE

colnames(r_squared) = c("city","month","value")
r_squared$Lead = "6 month" #CHANGE

write.csv(r_squared,'/Users/maxbeal/Desktop/PhD/Amazon/ModelResults/ModelR2_6mo_nb.csv') #CHANGE

```




```{r}


#cases = df1 %>% dplyr::select("cases","year","city","month")

df1 = read.csv('/Users/maxbeal/Desktop/PhD/Amazon/Data/LeadTimePreds/Predictors_3MLead_incidence')

df1 <- df1[complete.cases(df1), ]

df1$month = as.factor(df1$month)
df1$city = as.factor(df1$city)

df1=df1 %>% dplyr::select(-X)

df1[df1$cases<0,] = 0

pdf = pivot_wider(df1, names_from = variable, values_from = value)


remove_na_columns <- function(df, percent) {
  na_percent <- colMeans(is.na(df))
  columns_to_remove <- names(na_percent[na_percent > percent])
  df <- df[, !names(df) %in% columns_to_remove]
  return(df)
}

pdf = remove_na_columns(pdf,0.4)
pdf=na.omit(pdf)

year = pdf$year

pdf = pdf %>% dplyr::select(-year)


#df=df1[,c("year","month","variable","value","city")]
#pdf = pivot_wider(df, names_from = variable, values_from = value)
names(pdf)=str_replace_all(names(pdf), "[[:punct:]]", "")
names(pdf) <- gsub("³", "", names(pdf))
names(pdf) <- gsub("\\+", "", names(pdf))
names(pdf) <- gsub(" ", "", names(pdf))


#pdf=merge(cases,pdf)
pdf$cases = round(pdf$cases)
colnames(pdf)

model = brms::brm(cases ~ . + (1|city) + (1|month) , data = pdf, family = negbinomial, chains = 5,
                         iter = 3000, warmup = 1000,adapt_delta=0.9)


saveRDS(model, "all_VBD_brms.RDS")


```




```{r}


summary(model)
#plot(model)

fit1 <- add_criterion(model, "loo")
summary(fit1)

fit1

pp_check(model)

```

```{r}
library(tidybayes)

load("/Users/maxbeal/Desktop/PhD/Amazon/all_VBD_brms.RDS")

(model_fit <- model %>%
    add_predicted_draws(france3_mbrms) %>%  # adding the posterior distribution
    ggplot(aes(x = year, y = pop)) +  
    stat_lineribbon(aes(y = .prediction), .width = c(.95, .80, .50),  # regression line and CI
                    alpha = 0.5, colour = "black") +
    geom_point(data = France, colour = "darkseagreen4", size = 3) +   # raw data
    scale_fill_brewer(palette = "Greys") +
    ylab("Calidris canutus abundance\n") +  # latin name for red knot
    xlab("\nYear") +
    theme_bw() +
    theme(legend.title = element_blank(),
          legend.position = c(0.15, 0.85))
```

